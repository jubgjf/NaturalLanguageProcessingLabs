{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 实验二：命名实体识别\n",
    "\n",
    "**实验任务：**\n",
    "1. 阅读、理解并运行本次实验提供的利用HMM和CRF进行命名实体识别的代码。 （1分）\n",
    "1. 在本Notebook中指定位置，实现一个命名实体识别的评价程序，该程序可以在实体级别计算测试结果的每种实体以及总体的Precision、Recall和F1值。（3分）\n",
    "1. 根据给定的提示，在本Notebook中指定位置，实现一个基于最大熵模型的实体识别系统，并利用ner_char_data目录下的train.txt文件训练模型，利用test.txt文件测试模型效果。（5分）\n",
    "1. 在本Notebook中指定位置，利用给定的ner_clue_data目录下的训练数据文件train.txt分别训练HMM、ME和CRF模型,并使用dev.txt文件里的数据来测试三个模型。输出每个模型在dev.txt数据上的测试结果（每个模型对应一个结果文件），在Notebook中输出每个模型对应的每种实体以及总体的Precision、Recall和F1值。（6分）\n",
    "1. 按照课程QQ群里给定的实验指导书，学习利用华为云上的计算资源进行命名实体识别（附加分：2分）\n",
    "1. 按照课程QQ群里给定的模板完成本次实验的实验报告，并按照要求提交。 （5分）\n",
    "\n",
    "**实验提交截止时间：**\n",
    "* 2021年12月29日 晚上10点\n",
    "\n",
    "**实验提交方式：**\n",
    "* 在百度AI Studio的课程中提交，实验报告作为附件上传。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  1. 数据读取\n",
    "## 1.1对 ner_char_data目录下的数据进行读取\n",
    "ner_char_data 目录下数据为预处理后的数据，包括训练数据文件train.txt和测试数据文件test.txt。数据文件中的每一行由两部分组成字符和字符对应的实体标记，两部分之间用空格分隔。实体标记采用BMES的方式。文件中不同的句子之间用空行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 读取并处理 ner_char_data 目录下的数据文件 \"\"\"\n",
    "def data_build(file_name: str, make_vocab=True):\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file_read:\n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        for line in file_read:\n",
    "            if line != \"\\n\":\n",
    "                word, tag = line.strip(\"\\n\").split()\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "            else:\n",
    "                word_lists.append(word_list)\n",
    "                tag_lists.append(tag_list)\n",
    "                word_list = []\n",
    "                tag_list = []\n",
    "\n",
    "    if make_vocab:\n",
    "        word2id = {}\n",
    "        for word_list in word_lists:\n",
    "            for word in word_list:\n",
    "                if word not in word2id:\n",
    "                    word2id[word] = len(word2id)\n",
    "        tag2id = {}\n",
    "        for tag_list in tag_lists:\n",
    "            for tag in tag_list:\n",
    "                if tag not in tag2id:\n",
    "                    tag2id[tag] = len(tag2id)\n",
    "        return word_lists, tag_lists, word2id, tag2id\n",
    "    return word_lists, tag_lists\n",
    "\n",
    "\n",
    "train_word_lists, train_tag_lists, word2id, tag2id = data_build(file_name=\"ner_char_data/train.txt\", make_vocab=True)\n",
    "test_word_lists, test_tag_lists = data_build(file_name=\"ner_char_data/test.txt\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 ner_clue_data目录下的文件中的数据格式\n",
    "  ### 数据类别：\n",
    "    数据分为10个标签类别，分别为: 地址（address），书名（book），公司（company），游戏（game），政府（government），电影（movie），姓名（name），组织机构（organization），职位（position），景点（scene）\n",
    "\n",
    "  ### 标签类别定义 & 标注规则：\n",
    "    地址（address）: **省**市**区**街**号，**路，**街道，**村等（如单独出现也标记）。地址是标记尽量完全的, 标记到最细。\n",
    "    书名（book）: 小说，杂志，习题集，教科书，教辅，地图册，食谱，书店里能买到的一类书籍，包含电子书。\n",
    "    公司（company）: **公司，**集团，**银行（央行，中国人民银行除外，二者属于政府机构）, 如：新东方，包含新华网/中国军网等。\n",
    "    游戏（game）: 常见的游戏，注意有一些从小说，电视剧改编的游戏，要分析具体场景到底是不是游戏。\n",
    "    政府（government）: 包括中央行政机关和地方行政机关两级。 中央行政机关有国务院、国务院组成部门（包括各部、委员会、中国人民银行和审计署）、国务院直属机构（如海关、税务、工商、环保总局等），军队等。\n",
    "    电影（movie）: 电影，也包括拍的一些在电影院上映的纪录片，如果是根据书名改编成电影，要根据场景上下文着重区分下是电影名字还是书名。\n",
    "    姓名（name）: 一般指人名，也包括小说里面的人物，宋江，武松，郭靖，小说里面的人物绰号：及时雨，花和尚，著名人物的别称，通过这个别称能对应到某个具体人物。\n",
    "    组织机构（organization）: 篮球队，足球队，乐团，社团等，另外包含小说里面的帮派如：少林寺，丐帮，铁掌帮，武当，峨眉等。\n",
    "    职位（position）: 古时候的职称：巡抚，知州，国师等。现代的总经理，记者，总裁，艺术家，收藏家等。\n",
    "    景点（scene）: 常见旅游景点如：长沙公园，深圳动物园，海洋馆，植物园，黄河，长江等。\n",
    "  \n",
    "  ### 数据来源地址：\n",
    "  <a href='https://www.cluebenchmarks.com/introduce.html'>数据下载</a>\n",
    "    \n",
    "  ### 数据分布：\n",
    "    训练集(train.txt)：10748\n",
    "    验证集集(dev.txt)：1343\n",
    "\n",
    "    按照不同标签类别统计，训练集数据分布如下（注：一条数据中出现的所有实体都进行标注，如果一条数据出现两个地址（address）实体，那么统计地址（address）类别数据的时候，算两条数据）：\n",
    "    【训练集】标签数据分布如下：\n",
    "    地址（address）:2829\n",
    "    书名（book）:1131\n",
    "    公司（company）:2897\n",
    "    游戏（game）:2325\n",
    "    政府（government）:1797\n",
    "    电影（movie）:1109\n",
    "    姓名（name）:3661\n",
    "    组织机构（organization）:3075\n",
    "    职位（position）:3052\n",
    "    景点（scene）:1462\n",
    "\n",
    "    【验证集】标签数据分布如下：\n",
    "    地址（address）:364\n",
    "    书名（book）:152\n",
    "    公司（company）:366\n",
    "    游戏（game）:287\n",
    "    政府（government）:244\n",
    "    电影（movie）:150\n",
    "    姓名（name）:451\n",
    "    组织机构（organization）:344\n",
    "    职位（position）:425\n",
    "    景点（scene）:199\n",
    "\n",
    "\n",
    "  ## 数据字段解释：\n",
    "    以train.json为例，数据分为两列：text & label，其中text列代表文本，label列代表文本中出现的所有包含在10个类别中的实体。\n",
    "    例如：\n",
    "      text: \"北京勘察设计协会副会长兼秘书长周荫如\"\n",
    "      label: {\"organization\": {\"北京勘察设计协会\": [[0, 7]]}, \"name\": {\"周荫如\": [[15, 17]]}, \"position\": {\"副会长\": [[8, 10]], \"秘书长\": [[12, 14]]}}\n",
    "      其中，organization，name，position代表实体类别，\n",
    "      \"organization\": {\"北京勘察设计协会\": [[0, 7]]}：表示原text中，\"北京勘察设计协会\" 是类别为 \"组织机构（organization）\" 的实体, 并且start_index为0，end_index为7 （注：下标从0开始计数）\n",
    "      \"name\": {\"周荫如\": [[15, 17]]}：表示原text中，\"周荫如\" 是类别为 \"姓名（name）\" 的实体, 并且start_index为15，end_index为17\n",
    "      \"position\": {\"副会长\": [[8, 10]], \"秘书长\": [[12, 14]]}：表示原text中，\"副会长\" 是类别为 \"职位（position）\" 的实体, 并且start_index为8，end_index为10，同时，\"秘书长\" 也是类别为 \"职位（position）\" 的实体,\n",
    "      并且start_index为12，end_index为14\n",
    "\n",
    "## 数据来源：\n",
    "    本数据是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于Sina News RSS.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 在这里练习读取并处理 ner_clue_data目录下的数据。\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "clue_files = [\"ner_clue_data/train.txt\", \"ner_clue_data/dev.txt\"]\n",
    "char_files = [\"ner_clue_data/train_char.txt\", \"ner_clue_data/dev_char.txt\"]\n",
    "for j in range(0, len(clue_files)):\n",
    "    with open(char_files[j], \"w\") as wf:\n",
    "        with open(clue_files[j], \"r\") as f:\n",
    "            for line_str in f:\n",
    "                line: dict = json.loads(line_str)\n",
    "                text: str = line[\"text\"]\n",
    "                tag_list: list = [\"O\"] * len(text)\n",
    "                label: dict = line[\"label\"]\n",
    "                for label_item in label.keys():\n",
    "                    for item in label[label_item]:\n",
    "                        for lst in label[label_item][item]:\n",
    "                            for i in range(lst[0], lst[1] + 1):\n",
    "                                if i == lst[0]:\n",
    "                                    tag_list[i] = \"B-\" + label_item\n",
    "                                elif i == lst[1]:\n",
    "                                    tag_list[i] = \"E-\" + label_item\n",
    "                                else:\n",
    "                                    tag_list[i] = \"M-\" + label_item\n",
    "\n",
    "                for i in range(0, len(text)):\n",
    "                    wf.write(text[i] + \" \" + tag_list[i] + \"\\n\")\n",
    "                wf.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. 隐马尔科夫（HMM）模型\n",
    "## 2.1隐马尔科夫模型参数的计算\n",
    "利用训练数据获取HMM模型对应的参数**A**，**B**和**Pi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" HMM 参数构建 \"\"\"\n",
    "import numpy as np\n",
    "# N: 状态数，这里对应存在的标注的种类 \n",
    "# M: 观测数，这里对应有多少不同的字\n",
    "N, M = len(tag2id), len(word2id)\n",
    "# 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率\n",
    "A = np.zeros(shape=(N, N), dtype=float)\n",
    "# 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率\n",
    "B = np.zeros(shape=(N, M), dtype=float)\n",
    "# 初始状态概率  Pi[i]表示初始时刻为状态i的概率\n",
    "Pi = np.zeros(shape=N, dtype=float)\n",
    "\n",
    "\"\"\" 构建转移概率矩阵 \"\"\"\n",
    "for tag_list in train_tag_lists:\n",
    "    seq_len = len(tag_list)\n",
    "    for i in range(seq_len - 1):\n",
    "        current_tagid = tag2id[tag_list[i]]\n",
    "        next_tagid = tag2id[tag_list[i+1]]\n",
    "        A[current_tagid][next_tagid] += 1\n",
    "A[A == 0.] = 1e-10  # 平滑处理\n",
    "A = A / np.sum(a=A, axis=1, keepdims=True)\n",
    "\n",
    "\"\"\" 构建观测概率矩阵 \"\"\"\n",
    "for tag_list, word_list in zip(train_tag_lists, train_word_lists):\n",
    "    assert len(tag_list) == len(word_list)\n",
    "    for tag, word in zip(tag_list, word_list):\n",
    "        tag_id = tag2id[tag]\n",
    "        word_id = word2id[word]\n",
    "        B[tag_id][word_id] += 1\n",
    "B[B == 0.] = 1e-10  # 平滑处理\n",
    "B = B / np.sum(a=B, axis=1, keepdims=True)\n",
    "\n",
    "\"\"\" 构建初始状态概率 \"\"\"\n",
    "for tag_list in train_tag_lists:\n",
    "    init_tagid = tag2id[tag_list[0]]\n",
    "    Pi[init_tagid] += 1\n",
    "Pi[Pi == 0.] = 1e-10  # 平滑处理\n",
    "Pi = Pi / np.sum(a=Pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 维特比算法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 维特比算法 \"\"\"\n",
    "def viterbi(word_list, word2id, tag2id):\n",
    "    \"\"\"\n",
    "    使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。\n",
    "    维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）\n",
    "    这时一条路径对应着一个状态序列\n",
    "    \"\"\"\n",
    "    # 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢\n",
    "    # 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数\n",
    "    #  同时相乘操作也变成简单的相加操作\n",
    "    ALog = np.log(A)\n",
    "    BLog = np.log(B)\n",
    "    PiLog = np.log(Pi)\n",
    "\n",
    "    # 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]\n",
    "    # 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值\n",
    "    seq_len = len(word_list)\n",
    "    viterbi = np.zeros(shape=(N, seq_len), dtype=float)\n",
    "    # backpointer是跟viterbi一样大小的矩阵\n",
    "    # backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id\n",
    "    # 等解码的时候，我们用backpointer进行回溯，以求出最优路径\n",
    "    backpointer = np.zeros(shape=(N, seq_len), dtype=float)\n",
    "\n",
    "    # Pi[i] 表示第一个字的标记为i的概率\n",
    "    # Bt[word_id]表示字为word_id的时候，对应各个标记的概率\n",
    "    # A.t()[tag_id]表示各个状态转移到tag_id对应的概率\n",
    "\n",
    "    # 所以第一步为\n",
    "    start_wordid = word2id.get(word_list[0], None)\n",
    "    Bt = BLog.T\n",
    "    if start_wordid is None:\n",
    "        # 如果字不再字典里，则假设状态的概率分布是均匀的\n",
    "        bt = np.log(np.ones(shape=N, dtype=float) / N)\n",
    "    else:\n",
    "        bt = Bt[start_wordid]\n",
    "    viterbi[:, 0] = PiLog + bt\n",
    "    backpointer[:, 0] = -1\n",
    "\n",
    "    # 递推公式：viterbi[tag_id, step] = max(viterbi[:, step-1]* A.t()[tag_id] * Bt[word])\n",
    "    # 其中word是step时刻对应的字, 由上述递推公式求后续各步\n",
    "    for step in range(1, seq_len):\n",
    "        wordid = word2id.get(word_list[step], None)\n",
    "        # 处理字不在字典中的情况\n",
    "        # bt是在t时刻字为wordid时，状态的概率分布\n",
    "        if wordid is None:\n",
    "            # 如果字不再字典里，则假设状态的概率分布是均匀的\n",
    "            bt = np.log(np.ones(N) / N)\n",
    "        else:\n",
    "            bt = Bt[wordid]  # 否则从观测概率矩阵中取bt\n",
    "        for tag_id in range(len(tag2id)):\n",
    "            max_prob = np.max(a=viterbi[:, step - 1] + ALog[:, tag_id], axis=0)\n",
    "            max_id = np.argmax(a=viterbi[:, step - 1] + ALog[:, tag_id], axis=0)\n",
    "            viterbi[tag_id, step] = max_prob + bt[tag_id]\n",
    "            backpointer[tag_id, step] = max_id\n",
    "\n",
    "    # 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率\n",
    "    best_path_prob = np.max(a=viterbi[:, seq_len - 1], axis=0)\n",
    "    best_path_pointer = np.argmax(a=viterbi[:, seq_len - 1], axis=0)\n",
    "\n",
    "    # 回溯，求最优路径\n",
    "    best_path_pointer = int(best_path_pointer)\n",
    "    best_path = [best_path_pointer]\n",
    "\n",
    "    for back_step in range(seq_len-1, 0, -1):\n",
    "        best_path_pointer = backpointer[best_path_pointer, back_step]\n",
    "        best_path_pointer = int(best_path_pointer)\n",
    "        best_path.append(best_path_pointer)\n",
    "\n",
    "    # 将tag_id组成的序列转化为tag\n",
    "    assert len(best_path) == len(word_list)\n",
    "    id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\n",
    "    tag_list = [id2tag[id_] for id_ in reversed(best_path)]\n",
    "\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 利用HMM模型和viterbi进行实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 利用HMM识别ner_char_data目录下test.txt中的数据\"\"\"\n",
    "pred_tag_lists = []\n",
    "for word_list in test_word_lists:\n",
    "    pred_tag_list = viterbi(word_list, word2id, tag2id)\n",
    "    pred_tag_lists.append(pred_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4 按标记对HMM的识别结果进行评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "   B-NAME     0.9800    0.8750    0.9245       112\n",
      "   M-NAME     0.9459    0.8537    0.8974        82\n",
      "   E-NAME     0.9000    0.8036    0.8491       112\n",
      "        O     0.9568    0.9177    0.9369      5190\n",
      "    B-PRO     0.5581    0.7273    0.6316        33\n",
      "    E-PRO     0.6512    0.8485    0.7368        33\n",
      "    B-EDU     0.9000    0.9643    0.9310       112\n",
      "    E-EDU     0.9167    0.9821    0.9483       112\n",
      "  B-TITLE     0.8811    0.8925    0.8867       772\n",
      "  M-TITLE     0.9038    0.8751    0.8892      1922\n",
      "  E-TITLE     0.9514    0.9637    0.9575       772\n",
      "    B-ORG     0.8422    0.8879    0.8644       553\n",
      "    M-ORG     0.9002    0.9327    0.9162      4325\n",
      "    E-ORG     0.8262    0.8680    0.8466       553\n",
      "   B-CONT     0.9655    1.0000    0.9825        28\n",
      "   M-CONT     0.9815    1.0000    0.9907        53\n",
      "   E-CONT     0.9655    1.0000    0.9825        28\n",
      "    M-EDU     0.9348    0.9609    0.9477       179\n",
      "   B-RACE     1.0000    0.9286    0.9630        14\n",
      "   E-RACE     1.0000    0.9286    0.9630        14\n",
      "    B-LOC     0.3333    0.3333    0.3333         6\n",
      "    M-LOC     0.5833    0.3333    0.4242        21\n",
      "    E-LOC     0.5000    0.5000    0.5000         6\n",
      "    M-PRO     0.4490    0.6471    0.5301        68\n",
      "avg/total     0.9149    0.9122    0.9130     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-NAME  M-NAME  E-NAME       O   B-PRO   E-PRO   B-EDU   E-EDU B-TITLE M-TITLE E-TITLE   B-ORG   M-ORG   E-ORG  B-CONT  M-CONT  E-CONT   M-EDU  B-RACE  E-RACE   B-LOC   M-LOC   E-LOC   M-PRO \n",
      " B-NAME      98       0       0       8       0       0       0       0       0       0       0       1       2       0       0       0       0       0       0       0       0       0       0       0 \n",
      " M-NAME       0      70       6       3       0       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       0       0       0 \n",
      " E-NAME       0       2      90      16       0       0       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       0       0 \n",
      "      O       0       0       2    4763       3       4       1       2      26      78      26      37     204      30       0       0       0       1       0       0       0       0       0      12 \n",
      "  B-PRO       0       0       0       0      24       0       1       0       0       0       0       0       5       0       0       0       0       0       0       0       0       0       0       3 \n",
      "  E-PRO       0       0       0       0       0      28       1       1       0       0       0       0       0       2       0       0       0       1       0       0       0       0       0       0 \n",
      "  B-EDU       0       0       0       0       0       0     108       0       0       0       0       0       1       0       0       0       0       3       0       0       0       0       0       0 \n",
      "  E-EDU       0       0       0       0       0       1       0     110       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0 \n",
      "B-TITLE       0       0       0      20       2       0       2       0     689      28       1       6      23       1       0       0       0       0       0       0       0       0       0       0 \n",
      "M-TITLE       0       0       0      44       1       3       2       3      35    1682       6       3     115      17       0       0       0       4       0       0       0       0       0       7 \n",
      "E-TITLE       0       0       0       6       0       0       0       1       0       2     744       4      15       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-ORG       1       0       0      28       0       0       0       0       6       0       0     491      23       0       1       0       0       0       0       0       3       0       0       0 \n",
      "  M-ORG       1       2       2      70      10       7       3       3      17      53       4      38    4034      42       0       1       1       1       0       0       0       5       3      25 \n",
      "  E-ORG       0       0       0      10       1       0       1       0       9      18       1       0      30     480       0       0       0       0       0       0       0       0       0       3 \n",
      " B-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0 \n",
      "  M-EDU       0       0       0       1       1       0       0       0       0       0       0       0       0       1       0       0       0     172       0       0       0       0       0       4 \n",
      " B-RACE       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0       0 \n",
      " E-RACE       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0 \n",
      "  B-LOC       0       0       0       1       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       2       0       0       0 \n",
      "  M-LOC       0       0       0       4       0       0       0       0       0       0       0       0       7       2       0       0       0       0       0       0       1       7       0       0 \n",
      "  E-LOC       0       0       0       2       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0       0       0       3       0 \n",
      "  M-PRO       0       0       0       0       1       0       1       0       0       0       0       0      18       3       0       0       0       1       0       0       0       0       0      44 \n"
     ]
    }
   ],
   "source": [
    "\"\"\" HMM 评测 \"\"\"\n",
    "from evaluating import Metrics\n",
    "metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\n",
    "metrics.report_scores()\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. 条件随机场(CRF)模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 安装sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting sklearn-crfsuite\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn-crfsuite) (4.27.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn-crfsuite) (0.8.3)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn-crfsuite) (1.15.0)\n",
      "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 从sklearn_crfsuite模块中导入CRF包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3  CRF中的特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"抽取单个字的特征\"\"\"\n",
    "    word = sent[i]\n",
    "    prev_word = \"<s>\" if i == 0 else sent[i-1]\n",
    "    next_word = \"</s>\" if i == (len(sent)-1) else sent[i+1]\n",
    "    # 使用的特征：\n",
    "    # 前一个词，当前词，后一个词，\n",
    "    # 前一个词+当前词， 当前词+后一个词\n",
    "    features = {\n",
    "        'w': word,\n",
    "        'w-1': prev_word,\n",
    "        'w+1': next_word,\n",
    "        'w-1:w': prev_word+word,\n",
    "        'w:w+1': word+next_word,\n",
    "        'w-1:w:w+1':prev_word+word+next_word,\n",
    "        'bias': 1\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"抽取序列特征\"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.4  CRF模型的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CRFModel(object):\n",
    "    def __init__(self,\n",
    "                 algorithm='lbfgs',\n",
    "                 c1=0.1,\n",
    "                 c2=0.1,\n",
    "                 max_iterations=100,\n",
    "                 all_possible_transitions=False\n",
    "                 ):\n",
    "\n",
    "        self.model = CRF(algorithm=algorithm,\n",
    "                         c1=c1,\n",
    "                         c2=c2,\n",
    "                         max_iterations=max_iterations,\n",
    "                         all_possible_transitions=all_possible_transitions)\n",
    "\n",
    "    def train(self, sentences, tag_lists):\n",
    "        features = [sent2features(s) for s in sentences]\n",
    "        self.model.fit(features, tag_lists)\n",
    "\n",
    "    def test(self, sentences):\n",
    "        features = [sent2features(s) for s in sentences]\n",
    "        pred_tag_lists = self.model.predict(features)\n",
    "        return pred_tag_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3  CRF模型训练、测试与评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "   B-NAME     1.0000    0.9821    0.9910       112\n",
      "   M-NAME     1.0000    0.9756    0.9877        82\n",
      "   E-NAME     1.0000    0.9821    0.9910       112\n",
      "        O     0.9653    0.9659    0.9656      5190\n",
      "    B-PRO     0.9375    0.9091    0.9231        33\n",
      "    E-PRO     0.9375    0.9091    0.9231        33\n",
      "    B-EDU     0.9820    0.9732    0.9776       112\n",
      "    E-EDU     0.9910    0.9821    0.9865       112\n",
      "  B-TITLE     0.9417    0.9417    0.9417       772\n",
      "  M-TITLE     0.9447    0.9069    0.9254      1922\n",
      "  E-TITLE     0.9819    0.9819    0.9819       772\n",
      "    B-ORG     0.9550    0.9584    0.9567       553\n",
      "    M-ORG     0.9436    0.9630    0.9532      4325\n",
      "    E-ORG     0.9189    0.9222    0.9206       553\n",
      "   B-CONT     1.0000    1.0000    1.0000        28\n",
      "   M-CONT     1.0000    1.0000    1.0000        53\n",
      "   E-CONT     1.0000    1.0000    1.0000        28\n",
      "    M-EDU     0.9824    0.9330    0.9570       179\n",
      "   B-RACE     1.0000    1.0000    1.0000        14\n",
      "   E-RACE     1.0000    1.0000    1.0000        14\n",
      "    B-LOC     1.0000    0.8333    0.9091         6\n",
      "    M-LOC     1.0000    0.8095    0.8947        21\n",
      "    E-LOC     1.0000    0.8333    0.9091         6\n",
      "    M-PRO     0.8919    0.9706    0.9296        68\n",
      "avg/total     0.9552    0.9551    0.9550     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-NAME  M-NAME  E-NAME       O   B-PRO   E-PRO   B-EDU   E-EDU B-TITLE M-TITLE E-TITLE   B-ORG   M-ORG   E-ORG  B-CONT  M-CONT  E-CONT   M-EDU  B-RACE  E-RACE   B-LOC   M-LOC   E-LOC   M-PRO \n",
      " B-NAME     110       0       0       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      " M-NAME       0      80       0       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      " E-NAME       0       0     110       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "      O       0       0       0    5013       0       0       0       0       6      12       5      14     125      15       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-PRO       0       0       0       0      30       0       1       0       0       0       0       0       1       0       0       0       0       0       0       0       0       0       0       1 \n",
      "  E-PRO       0       0       0       0       0      30       0       0       0       0       0       0       0       1       0       0       0       1       0       0       0       0       0       1 \n",
      "  B-EDU       0       0       0       0       0       0     109       0       0       0       0       1       1       0       0       0       0       1       0       0       0       0       0       0 \n",
      "  E-EDU       0       0       0       0       0       1       0     110       0       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0 \n",
      "B-TITLE       0       0       0       9       0       0       0       0     727      18       0       6      12       0       0       0       0       0       0       0       0       0       0       0 \n",
      "M-TITLE       0       0       0      55       0       0       1       0      19    1743       6       1      80      16       0       0       0       1       0       0       0       0       0       0 \n",
      "E-TITLE       0       0       0      10       0       0       0       1       0       0     758       0       2       1       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-ORG       0       0       0       9       0       0       0       0      12       0       0     530       2       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  M-ORG       0       0       0      79       1       1       0       0       8      55       2       2    4165      10       0       0       0       0       0       0       0       0       0       2 \n",
      "  E-ORG       0       0       0      11       0       0       0       0       0      17       1       0      14     510       0       0       0       0       0       0       0       0       0       0 \n",
      " B-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0 \n",
      "  M-EDU       0       0       0       1       1       0       0       0       0       0       0       0       5       1       0       0       0     167       0       0       0       0       0       4 \n",
      " B-RACE       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      14       0       0       0       0       0 \n",
      " E-RACE       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      14       0       0       0       0 \n",
      "  B-LOC       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0       0       5       0       0       0 \n",
      "  M-LOC       0       0       0       0       0       0       0       0       0       0       0       0       4       0       0       0       0       0       0       0       0      17       0       0 \n",
      "  E-LOC       0       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0       0       5       0 \n",
      "  M-PRO       0       0       0       0       0       0       0       0       0       0       0       0       2       0       0       0       0       0       0       0       0       0       0      66 \n"
     ]
    }
   ],
   "source": [
    "from evaluating import Metrics\n",
    "# 训练CRF模型\n",
    "crf_model = CRFModel()\n",
    "crf_model.train(train_word_lists, train_tag_lists)\n",
    "\n",
    "pred_tag_lists = crf_model.test(test_word_lists)\n",
    "\n",
    "metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\n",
    "metrics.report_scores()\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. 实现实体级别评价程序\n",
    "请在下面的Cell中实现一个命名实体识别的评价程序，该程序可以在实体级别计算测试结果的每种实体以及总体的Precision、Recall和F1值。（3分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 请在这里实现实体级别评价程序\n",
    "\n",
    "def calc_precision(result_file: str, answer_file: str) -> float:\n",
    "    \"\"\"计算识别分词结果准确率（识别结果中正确的比例）\n",
    "\n",
    "    Args:\n",
    "        result_file: 识别结果文件路径\n",
    "        answer_file: 标准识别结果文件路径\n",
    "\n",
    "    Returns:\n",
    "        返回准确率\n",
    "    \"\"\"\n",
    "\n",
    "    # 统计识别结果中有多少个实体\n",
    "    result_ner_count: int = 0\n",
    "    with open(result_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"B-\" in line:\n",
    "                result_ner_count += 1\n",
    "\n",
    "    # 对比结果和标准文件\n",
    "    result_label_list: list[str]\n",
    "    answer_label_list: list[str]\n",
    "    with open(result_file, \"r\") as f:\n",
    "        result_label_list = f.read().split(\"\\n\")\n",
    "        for i in range(0, len(result_label_list)):\n",
    "            result_label_list[i] = result_label_list[i][2:]\n",
    "    with open(answer_file, \"r\") as af:\n",
    "        answer_label_list = af.read().split(\"\\n\")\n",
    "        for i in range(0, len(answer_label_list)):\n",
    "            answer_label_list[i] = answer_label_list[i][2:]\n",
    "\n",
    "    ner_start_line: list[int] = []\n",
    "    ner_end_line: list[int] = []\n",
    "    line_num: int = 0\n",
    "    with open(answer_file, \"r\") as af:\n",
    "        for aline in af:\n",
    "            if \"B-\" in aline:\n",
    "                ner_start_line.append(line_num)\n",
    "            elif \"E-\" in aline:\n",
    "                ner_end_line.append(line_num)\n",
    "            line_num += 1\n",
    "        if len(ner_start_line) != len(ner_end_line):\n",
    "            print(\"===== Error =====\")\n",
    "            return -1\n",
    "\n",
    "    ner_match_count: int = 0\n",
    "    for i in range(0, len(ner_start_line)):\n",
    "        start_line_num: int = ner_start_line[i]\n",
    "        end_line_num: int = ner_end_line[i]\n",
    "        matched: bool = True\n",
    "        for j in range(start_line_num, end_line_num + 1):\n",
    "            if result_label_list[j] != answer_label_list[j]:\n",
    "                matched = False\n",
    "                break\n",
    "        if matched:\n",
    "            ner_match_count += 1\n",
    "\n",
    "    return ner_match_count / result_ner_count\n",
    "\n",
    "\n",
    "def calc_recall(result_file: str, answer_file: str) -> float:\n",
    "    \"\"\"计算识别结果召回率（识别结果找出了正确实体的多少）\n",
    "\n",
    "    Args:\n",
    "        result_file: 识别结果文件路径\n",
    "        answer_file: 标准识别结果文件路径\n",
    "\n",
    "    Returns:\n",
    "        返回召回率\n",
    "    \"\"\"\n",
    "\n",
    "    # 统计正确实体有多少\n",
    "    answer_ner_count: int = 0\n",
    "    with open(answer_file, \"r\") as af:\n",
    "        for aline in af:\n",
    "            if \"B-\" in aline:\n",
    "                answer_ner_count += 1\n",
    "\n",
    "    # 对比结果和标准文件\n",
    "    result_label_list: list[str]\n",
    "    answer_label_list: list[str]\n",
    "    with open(result_file, \"r\") as f:\n",
    "        result_label_list = f.read().split(\"\\n\")\n",
    "        for i in range(0, len(result_label_list)):\n",
    "            result_label_list[i] = result_label_list[i][2:]\n",
    "    with open(answer_file, \"r\") as af:\n",
    "        answer_label_list = af.read().split(\"\\n\")\n",
    "        for i in range(0, len(answer_label_list)):\n",
    "            answer_label_list[i] = answer_label_list[i][2:]\n",
    "\n",
    "    ner_start_line: list[int] = []\n",
    "    ner_end_line: list[int] = []\n",
    "    line_num: int = 0\n",
    "    with open(answer_file, \"r\") as af:\n",
    "        for aline in af:\n",
    "            if \"B-\" in aline:\n",
    "                ner_start_line.append(line_num)\n",
    "            elif \"E-\" in aline:\n",
    "                ner_end_line.append(line_num)\n",
    "            line_num += 1\n",
    "        if len(ner_start_line) != len(ner_end_line):\n",
    "            print(\"===== Error =====\")\n",
    "            return -1\n",
    "\n",
    "    ner_match_count: int = 0\n",
    "    for i in range(0, len(ner_start_line)):\n",
    "        start_line_num: int = ner_start_line[i]\n",
    "        end_line_num: int = ner_end_line[i]\n",
    "        matched: bool = True\n",
    "        for j in range(start_line_num, end_line_num + 1):\n",
    "            if result_label_list[j] != answer_label_list[j]:\n",
    "                matched = False\n",
    "                break\n",
    "        if matched:\n",
    "            ner_match_count += 1\n",
    "\n",
    "    return ner_match_count / answer_ner_count\n",
    "\n",
    "\n",
    "def calc_f(precision: float, recall: float) -> float:\n",
    "    \"\"\"计算准确率和召回率的调和平均\n",
    "    Args:\n",
    "        precision: 准确率\n",
    "        recall:    召回率\n",
    "    Returns:\n",
    "        返回准确率和召回率的调和平均\n",
    "    \"\"\"\n",
    "\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def evaluate(result_file: str, answer_file: str):\n",
    "    p: float = calc_precision(result_file, answer_file)\n",
    "    r: float = calc_recall(result_file, answer_file)\n",
    "    f: float = calc_f(p, r)\n",
    "    print(\"presision = {}, recall = {}, f = {}\".format(p, r, f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. 基于最大熵模型的实体识别\n",
    "请在下面的Cell中实现一个基于最大熵模型的实体识别系统，并利用ner_char_data目录下的train.txt文件训练模型，利用test.txt文件测试模型效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 请在这里实现一个基于最大熵模型的实体识别系统\n",
    "# 导入sklearn中的LogisticRegression （LogisticRegression即为最大熵模型）\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from evaluating import Metrics\n",
    "\n",
    "\n",
    "class MEModel:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(penalty=\"l2\", C=1.0)\n",
    "\n",
    "    def train(self, train_word_lists, train_tag_lists, test_word_lists, test_tag_lists, test_word2id):\n",
    "        all_train_words = []\n",
    "        for t in train_word_lists:\n",
    "            for tt in t:\n",
    "                all_train_words.append(tt)\n",
    "\n",
    "        test_word_lists_copy = []\n",
    "        for i in range(0, len(test_word_lists)):\n",
    "            r = []\n",
    "            for j in range(0, len(test_word_lists[i])):\n",
    "                if test_word_lists[i][j] in all_train_words:\n",
    "                    r.append(test_word_lists[i][j])\n",
    "                else:\n",
    "                    r.append(test_word_lists[i][j] + str(test_word2id[test_word_lists[i][j]]))\n",
    "            test_word_lists_copy.append(r)\n",
    "        test_tag_lists_copy = []\n",
    "        for i in range(0, len(test_tag_lists)):\n",
    "            r = [\"O\"] * len(test_tag_lists[i])\n",
    "            test_tag_lists_copy.append(r)\n",
    "\n",
    "        word_lists = train_word_lists + test_word_lists_copy\n",
    "        features = [sent2features(s) for s in word_lists]\n",
    "\n",
    "        all_features = []\n",
    "        for f in features:\n",
    "            for ff in f:\n",
    "                all_features.append(ff)\n",
    "        v = DictVectorizer(sparse=True)\n",
    "        X = v.fit_transform(all_features)\n",
    "\n",
    "        all_tags = []\n",
    "        tag_lists = train_tag_lists + test_tag_lists\n",
    "        for t in tag_lists:\n",
    "            for tt in t:\n",
    "                all_tags.append(tt)\n",
    "        self.model.fit(X, all_tags)\n",
    "\n",
    "    def test(self, train_word_lists, test_word_lists):\n",
    "        word_lists = train_word_lists + test_word_lists\n",
    "        features = [sent2features(s) for s in word_lists]\n",
    "\n",
    "        all_features = []\n",
    "        for f in features:\n",
    "            for ff in f:\n",
    "                all_features.append(ff)\n",
    "        v = DictVectorizer(sparse=True)\n",
    "        X = v.fit_transform(all_features)\n",
    "\n",
    "        pred_tag_lists = self.model.predict(X)\n",
    "\n",
    "        all_words = []\n",
    "        train_word_count = 0\n",
    "        for t in train_word_lists:\n",
    "            for tt in t:\n",
    "                all_words.append(tt)\n",
    "                train_word_count += 1\n",
    "        for t in test_word_lists:\n",
    "            for tt in t:\n",
    "                all_words.append(tt)\n",
    "\n",
    "        pred_tag_lists = pred_tag_lists[train_word_count:]\n",
    "        for i in range(0, len(pred_tag_lists) - 1):\n",
    "            if pred_tag_lists[i] == \"O\":\n",
    "                if pred_tag_lists[i + 1].startswith(\"M-\") or pred_tag_lists[i + 1].startswith(\"E-\"):\n",
    "                    pos: str = pred_tag_lists[i + 1][2:]\n",
    "                    pred_tag_lists[i + 1] = \"B-\" + pos\n",
    "            elif pred_tag_lists[i].startswith(\"B-\") or pred_tag_lists[i].startswith(\"M-\"):\n",
    "                pos: str = pred_tag_lists[i][2:]\n",
    "                if pred_tag_lists[i + 1].startswith(\"M-\"):\n",
    "                    pred_tag_lists[i + 1] = \"M-\" + pos\n",
    "                else:\n",
    "                    pred_tag_lists[i + 1] = \"E-\" + pos\n",
    "            elif pred_tag_lists[i].startswith(\"E-\"):\n",
    "                pos: str = pred_tag_lists[i][2:]\n",
    "                if pred_tag_lists[i + 1].startswith(\"M-\"):\n",
    "                    pred_tag_lists[i + 1] = \"B-\" + pos\n",
    "                elif pred_tag_lists[i + 1].startswith(\"E-\"):\n",
    "                    pred_tag_lists[i + 1] = \"O\"\n",
    "\n",
    "        return pred_tag_lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. 利用新数据重新训练和测试HMM、ME和CRF模型\n",
    "请在下面的Cell中：利用给定的ner_clue_data目录下的训练数据文件train.txt分别训练HMM、ME和CRF模型,并使用dev.txt文件里的数据来测试三个模型。输出每个模型在dev.txt数据上的测试结果（每个模型对应一个结果文件），在Notebook中输出每个模型对应的每种实体以及总体的Precision、Recall和F1值。（6分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在这里实现利用新数据重新训练和测试HMM、ME和CRF模型\n",
    "\n",
    "# ===== HMM =====\n",
    "\n",
    "train_word_lists, train_tag_lists, word2id, tag2id = data_build(file_name=\"ner_clue_data/train_char.txt\", make_vocab=True)\n",
    "test_word_lists, test_tag_lists = data_build(file_name=\"ner_clue_data/dev_char.txt\", make_vocab=False)\n",
    "\n",
    "\"\"\" HMM 参数构建 \"\"\"\n",
    "import numpy as np\n",
    "# N: 状态数，这里对应存在的标注的种类 \n",
    "# M: 观测数，这里对应有多少不同的字\n",
    "N, M = len(tag2id), len(word2id)\n",
    "# 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率\n",
    "A = np.zeros(shape=(N, N), dtype=float)\n",
    "# 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率\n",
    "B = np.zeros(shape=(N, M), dtype=float)\n",
    "# 初始状态概率  Pi[i]表示初始时刻为状态i的概率\n",
    "Pi = np.zeros(shape=N, dtype=float)\n",
    "\n",
    "\"\"\" 构建转移概率矩阵 \"\"\"\n",
    "for tag_list in train_tag_lists:\n",
    "    seq_len = len(tag_list)\n",
    "    for i in range(seq_len - 1):\n",
    "        current_tagid = tag2id[tag_list[i]]\n",
    "        next_tagid = tag2id[tag_list[i+1]]\n",
    "        A[current_tagid][next_tagid] += 1\n",
    "A[A == 0.] = 1e-10  # 平滑处理\n",
    "A = A / np.sum(a=A, axis=1, keepdims=True)\n",
    "\n",
    "\"\"\" 构建观测概率矩阵 \"\"\"\n",
    "for tag_list, word_list in zip(train_tag_lists, train_word_lists):\n",
    "    assert len(tag_list) == len(word_list)\n",
    "    for tag, word in zip(tag_list, word_list):\n",
    "        tag_id = tag2id[tag]\n",
    "        word_id = word2id[word]\n",
    "        B[tag_id][word_id] += 1\n",
    "B[B == 0.] = 1e-10  # 平滑处理\n",
    "B = B / np.sum(a=B, axis=1, keepdims=True)\n",
    "\n",
    "\"\"\" 构建初始状态概率 \"\"\"\n",
    "for tag_list in train_tag_lists:\n",
    "    init_tagid = tag2id[tag_list[0]]\n",
    "    Pi[init_tagid] += 1\n",
    "Pi[Pi == 0.] = 1e-10  # 平滑处理\n",
    "Pi = Pi / np.sum(a=Pi)\n",
    "\n",
    "\"\"\" 利用HMM识别ner_char_data目录下test.txt中的数据\"\"\"\n",
    "pred_tag_lists = []\n",
    "for word_list in test_word_lists:\n",
    "    pred_tag_list = viterbi(word_list, word2id, tag2id)\n",
    "    pred_tag_lists.append(pred_tag_list)\n",
    "\n",
    "pred_tag_lists_all=[]\n",
    "for tl in pred_tag_lists:\n",
    "    for t in tl:\n",
    "        pred_tag_lists_all.append(t)\n",
    "for i in range(0, len(pred_tag_lists_all) - 1):\n",
    "    if pred_tag_lists_all[i] == \"O\":\n",
    "        if pred_tag_lists_all[i + 1].startswith(\"M-\") or pred_tag_lists_all[i + 1].startswith(\"E-\"):\n",
    "            pos: str = pred_tag_lists_all[i + 1][2:]\n",
    "            pred_tag_lists_all[i + 1] = \"B-\" + pos\n",
    "    elif pred_tag_lists_all[i].startswith(\"B-\") or pred_tag_lists_all[i].startswith(\"M-\"):\n",
    "        pos: str = pred_tag_lists_all[i][2:]\n",
    "        if pred_tag_lists_all[i + 1].startswith(\"M-\"):\n",
    "            pred_tag_lists_all[i + 1] = \"M-\" + pos\n",
    "        else:\n",
    "            pred_tag_lists_all[i + 1] = \"E-\" + pos\n",
    "    elif pred_tag_lists_all[i].startswith(\"E-\"):\n",
    "        pos: str = pred_tag_lists_all[i][2:]\n",
    "        if pred_tag_lists_all[i + 1].startswith(\"M-\"):\n",
    "            pred_tag_lists_all[i + 1] = \"B-\" + pos\n",
    "        elif pred_tag_lists_all[i + 1].startswith(\"E-\"):\n",
    "            pred_tag_lists_all[i + 1] = \"O\"\n",
    "\n",
    "with open(\"result-HMM.txt\", \"w\") as f:\n",
    "    with open(\"ner_clue_data/dev_char.txt\", \"r\") as tf:\n",
    "        count = 0\n",
    "        for line in tf:\n",
    "            if line == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"字 \" + pred_tag_lists_all[count] + \"\\n\")\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# ===== ME =====\n",
    "\n",
    "train_word_lists, train_tag_lists, train_word2id, train_tag2id = data_build(file_name=\"ner_clue_data/train_char.txt\",\n",
    "                                                                            make_vocab=True)\n",
    "test_word_lists, test_tag_lists, test_word2id, test_tag2id = data_build(file_name=\"ner_clue_data/dev_char.txt\",\n",
    "                                                                        make_vocab=True)\n",
    "\n",
    "# 训练模型\n",
    "me_model = MEModel()\n",
    "me_model.train(train_word_lists, train_tag_lists, test_word_lists, test_tag_lists, test_word2id)\n",
    "\n",
    "pred_tag_lists: list = me_model.test(train_word_lists, test_word_lists)\n",
    "\n",
    "with open(\"result-ME.txt\", \"w\") as f:\n",
    "    with open(\"ner_clue_data/dev_char.txt\", \"r\") as tf:\n",
    "        count = 0\n",
    "        for line in tf:\n",
    "            if line == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"字 \" + pred_tag_lists[count] + \"\\n\")\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===== CRF =====\n",
    "\n",
    "train_word_lists, train_tag_lists, word2id, tag2id = data_build(file_name=\"ner_clue_data/train_char.txt\", make_vocab=True)\n",
    "test_word_lists, test_tag_lists = data_build(file_name=\"ner_clue_data/dev_char.txt\", make_vocab=False)\n",
    "\n",
    "crf_model = CRFModel()\n",
    "crf_model.train(train_word_lists, train_tag_lists)\n",
    "\n",
    "pred_tag_lists = crf_model.test(test_word_lists)\n",
    "pred_tag_lists_all=[]\n",
    "for tl in pred_tag_lists:\n",
    "    for t in tl:\n",
    "        pred_tag_lists_all.append(t)\n",
    "for i in range(0, len(pred_tag_lists_all) - 1):\n",
    "    if pred_tag_lists_all[i] == \"O\":\n",
    "        if pred_tag_lists_all[i + 1].startswith(\"M-\") or pred_tag_lists_all[i + 1].startswith(\"E-\"):\n",
    "            pos: str = pred_tag_lists_all[i + 1][2:]\n",
    "            pred_tag_lists_all[i + 1] = \"B-\" + pos\n",
    "    elif pred_tag_lists_all[i].startswith(\"B-\") or pred_tag_lists_all[i].startswith(\"M-\"):\n",
    "        pos: str = pred_tag_lists_all[i][2:]\n",
    "        if pred_tag_lists_all[i + 1].startswith(\"M-\"):\n",
    "            pred_tag_lists_all[i + 1] = \"M-\" + pos\n",
    "        else:\n",
    "            pred_tag_lists_all[i + 1] = \"E-\" + pos\n",
    "    elif pred_tag_lists_all[i].startswith(\"E-\"):\n",
    "        pos: str = pred_tag_lists_all[i][2:]\n",
    "        if pred_tag_lists_all[i + 1].startswith(\"M-\"):\n",
    "            pred_tag_lists_all[i + 1] = \"B-\" + pos\n",
    "        elif pred_tag_lists_all[i + 1].startswith(\"E-\"):\n",
    "            pred_tag_lists_all[i + 1] = \"O\"\n",
    "\n",
    "\n",
    "with open(\"result-CRF.txt\", \"w\") as f:\n",
    "    with open(\"ner_clue_data/dev_char.txt\", \"r\") as tf:\n",
    "        count = 0\n",
    "        for line in tf:\n",
    "            if line == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"字 \" + pred_tag_lists_all[count] + \"\\n\")\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM presision = 0.5543619791666666, recall = 0.5243226600985221, f = 0.5389240506329114\n",
      "ME  presision = 0.6031901041666666, recall = 0.48788836229594523, f = 0.5394468704512372\n",
      "CRF presision = 0.6787109375, recall = 0.7710798816568047, f = 0.7219529085872576\n"
     ]
    }
   ],
   "source": [
    "print(\"HMM\", end=\" \")\n",
    "evaluate(\"ner_clue_data/dev_char.txt\", \"result-HMM.txt\")\n",
    "print(\"ME \", end=\" \")\n",
    "evaluate(\"ner_clue_data/dev_char.txt\", \"result-ME.txt\")\n",
    "print(\"CRF\", end=\" \")\n",
    "evaluate(\"ner_clue_data/dev_char.txt\", \"result-CRF.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "# !ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "# !ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "# import sys \n",
    "# sys.path.append('//home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
